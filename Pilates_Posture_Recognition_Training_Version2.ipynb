{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßò Pilates Posture Recognition - Complete Training Notebook\n",
    "\n",
    "### What This Notebook Does:\n",
    "1. **Collects** training data from videos/images of Pilates poses\n",
    "2. **Extracts** body landmarks (keypoints) using MediaPipe\n",
    "3. **Calculates** joint angles from those landmarks\n",
    "4. **Trains** a machine learning model to recognize different Pilates postures\n",
    "5. **Exports** the model to use in your Android app\n",
    "\n",
    "### No ML Experience Needed!\n",
    "Just run each cell in order and follow the instructions. üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Required Libraries\n",
    "\n",
    "**What this does:** Installs the tools we need:\n",
    "- `mediapipe`: Detects body poses\n",
    "- `opencv-python`: Processes videos/images\n",
    "- `tensorflow`: Trains our AI model\n",
    "- `scikit-learn`: Helps with machine learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (this may take 1-2 minutes)\n",
    "!pip install mediapipe opencv-python tensorflow scikit-learn matplotlib numpy pandas -q\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Import Libraries\n",
    "\n",
    "**What this does:** Loads all the tools we just installed so we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"MediaPipe version: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize MediaPipe Pose\n",
    "\n",
    "**What this does:** Sets up MediaPipe to detect body poses.\n",
    "\n",
    "MediaPipe will find 33 keypoints on the body (shoulders, hips, knees, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create a Pose detector\n",
    "# min_detection_confidence: How sure MediaPipe should be before detecting a pose (0.5 = 50%)\n",
    "# min_tracking_confidence: How sure it should be when tracking movement (0.5 = 50%)\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MediaPipe Pose initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Helper Functions - Calculate Angles\n",
    "\n",
    "**What this does:** Creates a function to calculate the angle between three body points.\n",
    "\n",
    "**Example:** The angle at your elbow is formed by your shoulder ‚Üí elbow ‚Üí wrist\n",
    "\n",
    "**Why angles?** Pilates postures are defined by specific body angles (bent knee, straight back, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(point_a, point_b, point_c):\n",
    "    \"\"\"\n",
    "    Calculate the angle at point_b formed by points a-b-c\n",
    "    \n",
    "    Args:\n",
    "        point_a, point_b, point_c: Each is a list [x, y] representing a body landmark\n",
    "    \n",
    "    Returns:\n",
    "        angle in degrees (0-180)\n",
    "    \n",
    "    Example: To find elbow angle:\n",
    "        - point_a = shoulder position\n",
    "        - point_b = elbow position (the joint we're measuring)\n",
    "        - point_c = wrist position\n",
    "    \"\"\"\n",
    "    # Convert points to numpy arrays for easier math\n",
    "    a = np.array(point_a)\n",
    "    b = np.array(point_b)\n",
    "    c = np.array(point_c)\n",
    "    \n",
    "    # Calculate vectors\n",
    "    ba = a - b  # Vector from b to a\n",
    "    bc = c - b  # Vector from b to c\n",
    "    \n",
    "    # Calculate angle using dot product formula\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    \n",
    "    # Ensure the value is in valid range for arccos\n",
    "    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
    "    \n",
    "    # Convert to degrees\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    angle_degrees = np.degrees(angle)\n",
    "    \n",
    "    return angle_degrees\n",
    "\n",
    "# Test the function\n",
    "test_angle = calculate_angle([0, 0], [1, 0], [1, 1])\n",
    "print(f\"‚úÖ Angle calculation function created! Test angle: {test_angle:.1f}¬∞ (should be 90¬∞)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Extract Features from Pose\n",
    "\n",
    "**What this does:** Calculates important angles for Pilates poses.\n",
    "\n",
    "**MediaPipe gives us 33 body landmarks. We'll calculate angles for:**\n",
    "- Left & right elbows\n",
    "- Left & right shoulders\n",
    "- Left & right hips\n",
    "- Left & right knees\n",
    "- Torso (spine) alignment\n",
    "\n",
    "These angles will be our \"features\" that the AI learns from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_features(landmarks):\n",
    "    \"\"\"\n",
    "    Extract important angles from MediaPipe pose landmarks\n",
    "    \n",
    "    Args:\n",
    "        landmarks: MediaPipe pose landmarks object\n",
    "    \n",
    "    Returns:\n",
    "        List of 10 angles (in degrees) representing the pose\n",
    "    \"\"\"\n",
    "    \n",
    "    # MediaPipe Landmark Indices (reference):\n",
    "    # 11 = Left Shoulder, 12 = Right Shoulder\n",
    "    # 13 = Left Elbow, 14 = Right Elbow\n",
    "    # 15 = Left Wrist, 16 = Right Wrist\n",
    "    # 23 = Left Hip, 24 = Right Hip\n",
    "    # 25 = Left Knee, 26 = Right Knee\n",
    "    # 27 = Left Ankle, 28 = Right Ankle\n",
    "    \n",
    "    # Extract coordinates [x, y] for each landmark we need\n",
    "    try:\n",
    "        # Left side\n",
    "        left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "        left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "        left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "        left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                   landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "        left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "        left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "        \n",
    "        # Right side\n",
    "        right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "        right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n",
    "        right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "        right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "        right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]\n",
    "        right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                      landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "        \n",
    "        # Calculate angles for important joints\n",
    "        # Format: calculate_angle(point_before, joint, point_after)\n",
    "        \n",
    "        angles = [\n",
    "            calculate_angle(left_shoulder, left_elbow, left_wrist),      # Left elbow angle\n",
    "            calculate_angle(right_shoulder, right_elbow, right_wrist),   # Right elbow angle\n",
    "            calculate_angle(left_elbow, left_shoulder, left_hip),        # Left shoulder angle\n",
    "            calculate_angle(right_elbow, right_shoulder, right_hip),     # Right shoulder angle\n",
    "            calculate_angle(left_shoulder, left_hip, left_knee),         # Left hip angle\n",
    "            calculate_angle(right_shoulder, right_hip, right_knee),      # Right hip angle\n",
    "            calculate_angle(left_hip, left_knee, left_ankle),            # Left knee angle\n",
    "            calculate_angle(right_hip, right_knee, right_ankle),         # Right knee angle\n",
    "            calculate_angle(left_hip, left_shoulder, right_shoulder),    # Left torso angle\n",
    "            calculate_angle(right_hip, right_shoulder, left_shoulder)    # Right torso angle\n",
    "        ]\n",
    "        \n",
    "        return angles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        # Return zeros if something goes wrong\n",
    "        return [0] * 10\n",
    "\n",
    "print(\"‚úÖ Feature extraction function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Prepare Your Training Data\n",
    "\n",
    "**IMPORTANT - READ THIS:**\n",
    "\n",
    "### How to Organize Your Training Data:\n",
    "\n",
    "1. **Create a folder** on your computer called `pilates_dataset`\n",
    "2. **Inside that folder**, create one subfolder for each Pilates pose:\n",
    "   - Example: `plank`, `bridge`, `hundred`, `roll_up`, etc.\n",
    "3. **Put images or videos** of each pose in its corresponding folder\n",
    "\n",
    "**Example Structure:**\n",
    "```\n",
    "pilates_dataset/\n",
    "‚îú‚îÄ‚îÄ plank/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ plank1.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ plank2.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ plank_video.mp4\n",
    "‚îú‚îÄ‚îÄ bridge/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ bridge1.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ bridge2.jpg\n",
    "‚îú‚îÄ‚îÄ hundred/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hundred1.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ hundred2.jpg\n",
    "‚îî‚îÄ‚îÄ roll_up/\n",
    "    ‚îú‚îÄ‚îÄ rollup1.jpg\n",
    "    ‚îî‚îÄ‚îÄ rollup2.jpg\n",
    "```\n",
    "\n",
    "### Then:\n",
    "- **Upload the entire `pilates_dataset` folder** to this Colab using the file browser on the left (üìÅ icon)\n",
    "- **OR** upload to Google Drive and mount it (instructions in next cell)\n",
    "\n",
    "### Minimum Requirements:\n",
    "- **At least 2-3 different poses** to start\n",
    "- **At least 10-20 images/frames per pose** (more is better!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Upload directly to Colab (simple, but files disappear when session ends)\n",
    "# Just drag and drop your 'pilates_dataset' folder to the Files panel on the left\n",
    "\n",
    "# OPTION 2: Use Google Drive (recommended - files persist)\n",
    "# Uncomment the lines below to mount Google Drive:\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATASET_PATH = '/content/drive/MyDrive/pilates_dataset'  # Adjust path as needed\n",
    "\n",
    "# For now, we'll assume direct upload:\n",
    "DATASET_PATH = '/content/pilates_dataset'\n",
    "\n",
    "print(f\"Dataset path set to: {DATASET_PATH}\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure you've uploaded your training data before running the next cells!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Process Images/Videos and Extract Angles\n",
    "\n",
    "**What this does:** \n",
    "- Scans through all your training images/videos\n",
    "- Detects the body pose in each one\n",
    "- Calculates the angles\n",
    "- Saves everything to a file\n",
    "\n",
    "**This may take a few minutes** depending on how many images you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Process all images/videos in the dataset and extract pose features\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the folder containing subfolders of poses\n",
    "    \n",
    "    Returns:\n",
    "        features: List of angle measurements (10 angles per sample)\n",
    "        labels: List of pose names corresponding to each sample\n",
    "    \"\"\"\n",
    "    features = []  # Will store our angle measurements\n",
    "    labels = []    # Will store the pose names\n",
    "    \n",
    "    # Supported file types\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png']\n",
    "    video_extensions = ['.mp4', '.avi', '.mov']\n",
    "    \n",
    "    # Check if dataset path exists\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"‚ùå Error: Dataset path '{dataset_path}' not found!\")\n",
    "        print(\"Please upload your training data first.\")\n",
    "        return [], []\n",
    "    \n",
    "    # Get list of pose folders (each folder = one pose type)\n",
    "    pose_folders = [f for f in os.listdir(dataset_path) \n",
    "                   if os.path.isdir(os.path.join(dataset_path, f))]\n",
    "    \n",
    "    if len(pose_folders) == 0:\n",
    "        print(f\"‚ùå Error: No pose folders found in '{dataset_path}'\")\n",
    "        return [], []\n",
    "    \n",
    "    print(f\"Found {len(pose_folders)} pose types: {pose_folders}\\n\")\n",
    "    \n",
    "    # Process each pose folder\n",
    "    for pose_name in pose_folders:\n",
    "        pose_folder_path = os.path.join(dataset_path, pose_name)\n",
    "        files = os.listdir(pose_folder_path)\n",
    "        \n",
    "        print(f\"Processing '{pose_name}' ({len(files)} files)...\")\n",
    "        samples_collected = 0\n",
    "        \n",
    "        # Process each file in the pose folder\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(pose_folder_path, filename)\n",
    "            file_ext = os.path.splitext(filename)[1].lower()\n",
    "            \n",
    "            # Process images\n",
    "            if file_ext in image_extensions:\n",
    "                image = cv2.imread(file_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                \n",
    "                # Convert BGR to RGB (MediaPipe uses RGB)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Detect pose\n",
    "                results = pose.process(image_rgb)\n",
    "                \n",
    "                # If pose detected, extract features\n",
    "                if results.pose_landmarks:\n",
    "                    angles = extract_pose_features(results.pose_landmarks.landmark)\n",
    "                    features.append(angles)\n",
    "                    labels.append(pose_name)\n",
    "                    samples_collected += 1\n",
    "            \n",
    "            # Process videos (extract frames)\n",
    "            elif file_ext in video_extensions:\n",
    "                cap = cv2.VideoCapture(file_path)\n",
    "                frame_count = 0\n",
    "                \n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    \n",
    "                    # Process every 10th frame to avoid too much duplicate data\n",
    "                    if frame_count % 10 == 0:\n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        results = pose.process(frame_rgb)\n",
    "                        \n",
    "                        if results.pose_landmarks:\n",
    "                            angles = extract_pose_features(results.pose_landmarks.landmark)\n",
    "                            features.append(angles)\n",
    "                            labels.append(pose_name)\n",
    "                            samples_collected += 1\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                \n",
    "                cap.release()\n",
    "        \n",
    "        print(f\"  ‚úÖ Collected {samples_collected} samples from '{pose_name}'\\n\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total samples collected: {len(features)}\")\n",
    "    print(f\"Total poses: {len(set(labels))}\")\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# Run the processing\n",
    "print(\"Starting dataset processing...\\n\")\n",
    "X, y = process_dataset(DATASET_PATH)\n",
    "\n",
    "if len(X) > 0:\n",
    "    print(\"\\n‚úÖ Dataset processed successfully!\")\n",
    "    print(f\"Shape of features: {np.array(X).shape}\")\n",
    "    print(f\"Number of samples per pose:\")\n",
    "    for pose in set(y):\n",
    "        print(f\"  - {pose}: {y.count(pose)} samples\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No data collected. Please check your dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Prepare Data for Training\n",
    "\n",
    "**What this does:**\n",
    "- Converts pose names to numbers (computers work with numbers, not text)\n",
    "- Splits data into **training set** (80%) and **test set** (20%)\n",
    "- Training set: Used to teach the model\n",
    "- Test set: Used to check if the model learned correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)  # Features (angles)\n",
    "y = np.array(y)  # Labels (pose names)\n",
    "\n",
    "# Create a mapping from pose names to numbers\n",
    "# Example: {'plank': 0, 'bridge': 1, 'hundred': 2}\n",
    "pose_classes = sorted(list(set(y)))\n",
    "pose_to_number = {pose: idx for idx, pose in enumerate(pose_classes)}\n",
    "number_to_pose = {idx: pose for pose, idx in pose_to_number.items()}\n",
    "\n",
    "print(\"Pose to Number Mapping:\")\n",
    "for pose, num in pose_to_number.items():\n",
    "    print(f\"  {pose} ‚Üí {num}\")\n",
    "\n",
    "# Convert pose names to numbers\n",
    "y_numeric = np.array([pose_to_number[pose] for pose in y])\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "# random_state=42 ensures we get the same split every time (for reproducibility)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_numeric, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,\n",
    "    stratify=y_numeric  # Ensures balanced split across all poses\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data split complete!\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Number of features (angles): {X_train.shape[1]}\")\n",
    "print(f\"Number of pose classes: {len(pose_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Train the Model (Option 1 - Random Forest)\n",
    "\n",
    "**What this does:** Trains a \"Random Forest\" classifier.\n",
    "\n",
    "**What is Random Forest?**\n",
    "- Think of it like asking 100 people (trees) to vote on what pose it is\n",
    "- Each person looks at the angles and makes a guess\n",
    "- The final answer is whatever most people voted for\n",
    "- Very accurate and beginner-friendly!\n",
    "\n",
    "**This is the recommended starting point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier\n",
    "# n_estimators=100 means we use 100 \"trees\" (voters)\n",
    "# random_state=42 ensures reproducibility\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,    # Number of trees\n",
    "    random_state=42,\n",
    "    max_depth=10,        # Prevents overfitting\n",
    "    n_jobs=-1            # Use all CPU cores for speed\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "print(\"This may take 10-30 seconds...\\n\")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "\n",
    "# Test the model on training data\n",
    "train_predictions = rf_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "# Test the model on test data (data it hasn't seen before)\n",
    "test_predictions = rf_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f\"üìä Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show detailed results\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test, test_predictions, \n",
    "                          target_names=pose_classes))\n",
    "\n",
    "# Feature importance (which angles matter most?)\n",
    "feature_names = [\n",
    "    'Left Elbow', 'Right Elbow',\n",
    "    'Left Shoulder', 'Right Shoulder',\n",
    "    'Left Hip', 'Right Hip',\n",
    "    'Left Knee', 'Right Knee',\n",
    "    'Left Torso', 'Right Torso'\n",
    "]\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nüîç Most Important Angles for Classification:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Train the Model (Option 2 - Neural Network)\n",
    "\n",
    "**What this does:** Trains a deep learning model (neural network).\n",
    "\n",
    "**When to use this:**\n",
    "- You have lots of data (500+ samples)\n",
    "- Random Forest isn't accurate enough\n",
    "- You want to experiment with deep learning\n",
    "\n",
    "**You can skip this if Random Forest works well!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding (required for neural networks)\n",
    "# Example: pose 0 becomes [1, 0, 0], pose 1 becomes [0, 1, 0]\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=len(pose_classes))\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=len(pose_classes))\n",
    "\n",
    "# Build the neural network\n",
    "nn_model = tf.keras.Sequential([\n",
    "    # Input layer - receives 10 angle values\n",
    "    tf.keras.layers.Input(shape=(10,)),\n",
    "    \n",
    "    # First hidden layer - 64 neurons\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Prevents overfitting\n",
    "    \n",
    "    # Second hidden layer - 32 neurons\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Output layer - one neuron per pose\n",
    "    tf.keras.layers.Dense(len(pose_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Neural Network Architecture:\")\n",
    "nn_model.summary()\n",
    "\n",
    "print(\"\\nTraining Neural Network...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    X_train, y_train_onehot,\n",
    "    epochs=50,              # Number of times to go through the data\n",
    "    batch_size=32,          # Number of samples per update\n",
    "    validation_split=0.2,   # Use 20% of training data for validation\n",
    "    verbose=1               # Show progress\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = nn_model.evaluate(X_test, y_test_onehot, verbose=0)\n",
    "\n",
    "print(f\"\\n‚úÖ Neural Network Training Complete!\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Convert Model to TensorFlow Lite (For Android)\n",
    "\n",
    "**What this does:** Converts your trained model to a format that Android can use.\n",
    "\n",
    "**TensorFlow Lite (.tflite) files:**\n",
    "- Much smaller size\n",
    "- Optimized for mobile devices\n",
    "- Fast inference on phones\n",
    "\n",
    "**We'll convert the Neural Network model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Random Forest as pickle (for reference)\n",
    "with open('pilates_rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "with open('pose_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump({'pose_to_number': pose_to_number, 'number_to_pose': number_to_pose}, f)\n",
    "\n",
    "print(\"‚úÖ Random Forest saved as 'pilates_rf_model.pkl'\")\n",
    "print(\"‚úÖ Pose mappings saved as 'pose_mapping.pkl'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Neural Network to TFLite (RECOMMENDED FOR ANDROID)\n",
    "\n",
    "# Create TFLite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(nn_model)\n",
    "\n",
    "# Apply optimizations to make the model smaller and faster\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert the model\n",
    "print(\"Converting Neural Network to TFLite format...\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "tflite_model_path = 'pilates_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"\\n‚úÖ TFLite model saved as '{tflite_model_path}'\")\n",
    "print(f\"Model size: {len(tflite_model) / 1024:.2f} KB\")\n",
    "\n",
    "# Test the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nüì± TFLite Model Details (for Android integration):\")\n",
    "print(f\"Input shape: {input_details[0]['shape']}\")\n",
    "print(f\"Input type: {input_details[0]['dtype']}\")\n",
    "print(f\"Output shape: {output_details[0]['shape']}\")\n",
    "print(f\"Output type: {output_details[0]['dtype']}\")\n",
    "\n",
    "# Test inference\n",
    "test_sample = X_test[0:1].astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
    "interpreter.invoke()\n",
    "tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "predicted_class = np.argmax(tflite_output[0])\n",
    "confidence = tflite_output[0][predicted_class]\n",
    "\n",
    "print(f\"\\nüß™ Test Prediction:\")\n",
    "print(f\"Predicted pose: {number_to_pose[predicted_class]}\")\n",
    "print(f\"Confidence: {confidence * 100:.2f}%\")\n",
    "print(f\"Actual pose: {number_to_pose[y_test[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Save Pose Labels (Important for Android!)\n",
    "\n",
    "**What this does:** Saves the mapping between numbers and pose names.\n",
    "\n",
    "**Why?** Your Android app needs to know:\n",
    "- 0 = \"plank\"\n",
    "- 1 = \"bridge\"\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labels as a text file\n",
    "labels_file = 'labels.txt'\n",
    "with open(labels_file, 'w') as f:\n",
    "    for pose in pose_classes:\n",
    "        f.write(pose + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Labels saved to '{labels_file}'\")\n",
    "print(\"\\nContents:\")\n",
    "with open(labels_file, 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Download both 'pilates_model.tflite' AND 'labels.txt' for your Android app!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Download Your Models\n",
    "\n",
    "**What this does:** Prepares all files for download.\n",
    "\n",
    "**Files you need for Android:**\n",
    "1. `pilates_model.tflite` - The trained model\n",
    "2. `labels.txt` - Pose name mappings\n",
    "\n",
    "**Click the folder icon** üìÅ on the left, then right-click each file ‚Üí Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Files ready for download:\")\n",
    "print(\"\\nüì¶ For Android App:\")\n",
    "if os.path.exists('pilates_model.tflite'):\n",
    "    print(\"  ‚úÖ pilates_model.tflite\")\n",
    "else:\n",
    "    print(\"  ‚ùå pilates_model.tflite (train neural network first!)\")\n",
    "\n",
    "if os.path.exists('labels.txt